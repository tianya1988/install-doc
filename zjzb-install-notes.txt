测试环境;无外网，啧啧啧！无root啧啧啧！

=========测试环境zookeeper=============
192.168.160.40
包传到/data目录下

cd /data
mkdir /data/soft-package

tar -zxvf apache-zookeeper-3.6.2-bin.tar.gz
mv /data/apache-zookeeper-3.6.2-bin /data/zookeeper

mkdir -p /data/softdata/zookeeper/data
mkdir -p /data/softdata/zookeeper/logs

cp /data/zookeeper/conf/zoo_sample.cfg  /data/zookeeper/conf/zoo.cfg

vim /data/zookeeper/conf/zoo.cfg
添加如下配置：
    dataDir=/data/softdata/zookeeper/data
    dataLogDir=/data/softdata/zookeeper/logs

#集群环境(测试环境不需要)
#    server.0=192.168.160.39:2888:3888
#    server.1=192.168.160.40:2888:3888
#    server.2=192.168.160.41:2888:3888

#集群环境(测试环境不需要)
touch /data/softdata/zookeeper/data/myid
echo "0" > /data/softdata/zookeeper/data/myid

sudo vim /etc/profile
    export ZK_HOME=/data/zookeeper
    export PATH=$PATH:$ZK_HOME/bin

启动：  systemctl start zookeeper
停止：  systemctl stop zookeeper
状态：  systemctl status zookeeper

===============测试环境kafka=================
192.168.160.40
tar -zxvf kafka_2.12-2.6.0.tgz
mv kafka_2.12-2.6.0 kafka

mkdir /data/softdata/kafka/kafka-logs
mkdir /data/softdata/kafka/logs

vim /data/kafka/config/server.properties

    log.dirs=/data/softdata/kafka/kafka-logs
    num.partitions=3
    #视
    #zookeeper.connect=192.168.160.40:2181

mkdir /data/softdata/kafka/logs
vim /data/kafka/bin/kafka-run-class.sh
    #195行
    LOG_DIR=/data/softdata/kafka/logs


启动：  systemctl start kafka
停止：  systemctl stop kafka
状态：  systemctl status kafka

=============测试环境es===========
192.168.160.43

mkdir -p /data/softdata/es/data
mkdir -p /data/softdata/es/logs

/home/app/elasticsearch-7.9.3/config


sudo echo '* soft nofile 655360'>> /etc/security/limits.conf
sudo echo '* hard nofile 655360'>> /etc/security/limits.conf
sudo echo '* hard nproc  40960'>> /etc/security/limits.conf
sudo echo '* soft nproc  40960'>> /etc/security/limits.conf
sudo echo 'es soft memlock unlimited' >> /etc/security/limits.conf
sudo echo 'es hard memlock unlimited' >> /etc/security/limits.conf

sudo echo 'vm.swappiness=1' >> /etc/sysctl.conf
sudo echo 'vm.max_map_count=262144' >> /etc/sysctl.conf


====redis=====
[base]
name=CentOS-$releasever - Base
baseurl=file:///data/soft/Centos/
gpgcheck=0
enabled=1


nohup ./src/redis-server > redis.log 2>&1 &





===== mysql ======
 192.168.160.45  3306

mysql：  检查数据库防火墙,先关闭
mysql -uroot -p
Dev123!@#

/data/mysql_5.7.32
/data/mysql_5.7.32/data
/etc/my.cnf
/usr/local/mysql/mysql-error.log

sudo service mysqld start
sudo service mysqld stop
sudo service mysqld status


==========================================================
192.168.160.39:9876


cd /data/rocketmq_4.7.1/bin
nohup sh mqnamesrv -n 192.168.160.39:9876 > nameServer.log 2>&1 &

cd /data/rocketmq_4.7.1
nohup sh bin/mqbroker -n 192.168.160.39:9876 -c conf/broker.properties autoCreateTopicEnable=true > broker.log 2>&1 &


/data/rocketmq_4.7.1/data


===========================================================
192.168.160.44
httpd
/etc/httpd/conf
/data/install-package/yum



======================================= k8s 开发 =====================================
10.1.20.103
10.1.20.106
10.1.20.107

10.1.20.108:80  harbor地址    admin  Harbor12345



======================================= k8s 测试 =====================================
192.168.160.125:11180  harbor地址  admin  Harbor12345

mysql：  检查数据库防火墙
mysql  192.168.160.2    root/Atest123

机器ip：
192.168.160.8   k8s-test-node01
192.168.160.9   k8s-test-node02
192.168.160.10  k8s-test-node03

####################### begin #######################
配置/etc/hosts
192.168.160.8   k8s-test-node01
192.168.160.9   k8s-test-node02
192.168.160.10  k8s-test-node03

#关闭防火墙
sudo systemctl stop firewalld
sudo systemctl disable firewalld
sudo sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
sudo setenforce 0

分别执行下述命令，检查防火墙状态及配置
sudo systemctl status firewalld.service
sudo iptables -L
sudo getenforce


2.4	设置 系统Limits限制
sudo su -   #(临时切到root权限，否则前三条权限不够)
    echo -e "*\thard\tnofile\t1048576" >> /etc/security/limits.conf
    echo -e "*\tsoft\tnofile\t655350" >> /etc/security/limits.conf
    echo -e "*\tsoft\tcore\t10000000" >> /etc/security/limits.conf
    /bin/sed -i 's/nproc     4096/nproc     65535/g' /etc/security/limits.d/20-nproc.conf
    /bin/sed -i 's/#DefaultLimitNOFILE=/DefaultLimitNOFILE=655350/g' /etc/systemd/system.conf
    /bin/sed -i 's/#DefaultLimitNPROC=/DefaultLimitNPROC=65535/g' /etc/systemd/system.conf
exit  #退出root用户


2.5	禁用SWAP
sudo su -   #(临时切到root权限)
    swapoff -a
    /bin/sed -i "s/.*swap.*/#&/" /etc/fstab
    /bin/sed -i "s/.*swappiness.*/#&/" /etc/sysctl.conf
    /bin/echo -e "vm.swappiness = 0" >> /etc/sysctl.conf

    sysctl -p
exit  #退出root用户

2.6	开启IP转发
sudo su -   #(临时切到root权限)
#必须顶格写
cat >> /etc/sysctl.conf << EOF
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl -p
exit  #退出root用户


#安装docker
unzip docker1903.zip 
cd docker1903
sudo rpm -ivh *.rpm --force --nodeps


第二步：配置docker registry
sudo su -
mkdir -p /etc/docker/
touch /etc/docker/daemon.json

#开发、测试、准生产如下：
cat > /etc/docker/daemon.json<<EOF
{
"registry-mirrors": [
    "https://bxsfpjcb.mirror.aliyuncs.com"
],
"max-concurrent-downloads": 10,
"log-driver": "json-file",
"log-level": "warn",
"log-opts": {
    "max-size": "10m",
    "max-file": "3"
    },
"insecure-registries": ["127.0.0.1","192.168.160.125:11180"],
"data-root":"/var/lib/docker"
}
EOF

#生产环境如下：
cat > /etc/docker/daemon.json<<EOF
{
"max-concurrent-downloads": 10,
"log-driver": "json-file",
"log-level": "warn",
"log-opts": {
    "max-size": "10m",
    "max-file": "3"
    },
"insecure-registries": ["127.0.0.1","192.168.160.125:11180"],
"data-root":"/data/var/lib/docker"
}
EOF


    groupadd docker
    usermod -aG docker app
    systemctl restart docker
exit

sudo docker login 192.168.160.125:11180
sudo docker push 192.168.160.125:11180/public/xxl-job:1102

#查看harbor地址中public项目是否存在
curl -u "admin:Harbor12345" -XGET -H "Content-Type:application/json" "http://192.168.160.125:11180/api/projects"
没有public，则创建
curl -u "admin:Harbor12345" -X POST -H "Content-Type: application/json" "http://192.168.160.125:11180/api/projects" -d @createproject.json
 cat createproject.json 
 {
  "project_name": "public",
  "public": 1
 }


2.8	配置jdk环境
#第一步：解压文件到对应目录
#tar -zxvf /data/install-tools/jdk-8u131-linux-x64.tar.gz -C /usr/java/
  
第二步：配置Java环境变量
#vim /etc/profile
    在最后一行加入
    export JAVA_HOME=/data/jdk1.8.0_131
    export JRE_HOME=${JAVA_HOME}/jre
    export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib:$CLASSPATH
    export JAVA_PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin
    export PATH=$PATH:${JAVA_PATH}

    #生产环境如下:
    export JAVA_HOME=/data/java
    export JRE_HOME=${JAVA_HOME}/jre
    export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib:$CLASSPATH
    export JAVA_PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin
    export PATH=$PATH:${JAVA_PATH}

第三步：加载环境变量
#source /etc/profile

查看系统变量软连接，如果不对，则删除重建：
    ll /usr/bin/java
    rm /usr/bin/java
    sudo ln -s  /data/jdk1.8.0_131/bin/java  /usr/bin/java
    
    #生产环境
    sudo ln -s  /data/java/bin/java  /usr/bin/java


# 检查ntp服务是否正常
ntpq -p

     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
*192.168.2.91    185.255.55.20    3 u  418 1024  377    1.206    1.033   0.799
 LOCAL(0)        .LOCL.          10 l  12d   64    0    0.000    0.000   0.000


4.2	部署集群环境
上传安装 k8s-install.zip

chmod 777 /data/ccbscf
sudo cp /data/ccbscf /usr/bin/sealos

#包里已有installk8s.sh，则跳过此步骤
touch installk8s.sh
vim installk8s.sh
        #!/bin/bash
        sudo sealos  $@  >> /data/k8sinstall.log 2>&1
        MYFILE=/data/k8sinstall.log
        ERRCODE=`tail $MYFILE | grep  '98488045' | wc -l `
        NUM=17
        MAX=`sed -n '$=' $MYFILE `
        if [ $ERRCODE == 1 ];then 
            let SLINE=MAX-NUM+1
            sudo sed -i $SLINE',$d' $MYFILE
            sudo echo "successd"
        else
            sudo echo "Install Failed ！ See /data/k8sinstall.log"
        fi

node1 需要与node1 node2 node3 打通ssh免秘钥登录

    ssh-keygen
    touch authorized_keys
    将所有机器的id_rsa.pub 全部放置到authorized_keys
    chmod 700 ~/.ssh
    chmod 600 ~/.ssh/authorized_keys

    cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bak
    vim /etc/ssh/sshd_config
        PermitRootLogin yes   #修改
        StrictModes no        #去掉注解，并修改
        RSAAuthentication yes #添加
        PubkeyAuthentication yes #添加

    systemctl restart sshd

sh installk8s.sh init --master 192.168.160.8 \
    --node 192.168.160.9 \
    --node 192.168.160.10 \
    --version v1.19.0 \
    --pkg-url /data/kube1.19.0.tar.gz

输出"successd"，代表安装成功

验证：
kubectl get cs
kubectl get pods -n kube-system
    

#如果出现错误可以清除后重装，命令如下：
    sh installk8s.sh clean --all


vim /etc/kubernetes/manifests/kube-controller-manager.yaml
注释 - --port=0
#    - --port=0

vim /etc/kubernetes/manifests/kube-scheduler.yaml
注释 - --port=0
#    - --port=0


systemctl restart kubelet


docker load -i apollo.tar
docker load -i xxl-job.tar

docker tag e3cbbab10a6e   192.168.160.125:11180/public/apollo:v1.3
docker rmi 12.0.217.11:11180/public/apollo:v1.3
docker tag 66e7e874a96e 192.168.160.125:11180/public/xxl-job:1102
docker rmi 12.0.217.11:11180/public/xxl-job:1102

数据库机器导入初始化脚本：
    source /data/sql/createuserapollo.sql
    source /data/sql/createuserxxl.sql
    source /data/sql/initconfig.sql
    source /data/sql/initportal.sql
    source /data/sql/xxl-1.1.sql


修改数据库地址:
    vim apollo-dev.yml
        
        修改镜像地址以及：

        env:
        - name: DEV_DB
          value: jdbc:mysql://192.168.160.2:3306/ApolloConfigDB?characterEncoding=utf8
        - name: DEV_DB_USER
          value: apollo
        - name: DEV_DB_PWD
          value: "jxrt@123"
        - name: DEV_LB
          value: apollosvc

    vim apollo-portal.yml

        修改镜像地址以及：

            image: 192.168.160.125:11180/public/apollo:v1.3

        env:
        - name: PORTAL_DB
          value: jdbc:mysql://192.168.160.2:3306/ApolloPortalDB?characterEncoding=utf8
        - name: PORTAL_DB_USER
          value: apollo
        - name: PORTAL_DB_PWD
          value: "jxrt@123"
        - name: DEV_URL
          value: http://apollosvc:8080

    mv ccbscfconfig.yml zjzbconfig.yml
    vim zjzbconfig.yml
        mysql_url: "jdbc:mysql://192.168.160.2:3306/xxljob?useUnicode=true&characterEncoding=UTF-8&useSSL=false"
        mysql_user: root
        mysql_password: "Atest123"

        #生产环境如下:
        mysql_url: "jdbc:mysql://192.168.161.3:3306/xxljob?useUnicode=true&characterEncoding=UTF-8&useSSL=false"
        mysql_user: "xxljob"
        mysql_password: "xxljob"
 
    vim xxl.yml
        修改镜像地址

启动服务：
    sudo kubectl apply -f /data/k8s-yaml/apollo-dev.yml
    sudo kubectl apply -f /data/k8s-yaml/apollo-portal.yml

    sudo kubectl apply -f /data/k8s-yaml/zjzbconfig.yml
    sudo kubectl apply -f /data/k8s-yaml/xxl.yml

查看服务状态：
[app@k8s-pre-node01 k8s-yaml]$ sudo kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
apollo-dev-6b58f568d4-lq6qd      1/1     Running   0          6m36s
apollo-portal-5947966f76-w8wzl   1/1     Running   0          6m28s
xxl-job-admin-6df4b8564c-zxvw9   1/1     Running   0          6m11s
查看某pod的日志：
    sudo kubectl logs  apollo-dev-5b5974c769-7hbcp

重启集群：
systemctl restart docker  #因为k8s中的服务都是在docker中部署的。
systemctl restart kubelet  #此操作可以不执行



#恢复sshd配置文件

cd /root/.ssh/ && mv authorized_keys authorized_keys.bak

cp /etc/ssh/sshd_config /etc/ssh/sshd_config.nopw
cp /etc/ssh/sshd_config.bak /etc/ssh/sshd_config

systemctl restart sshd



============================ k8s 预生产 ==============================
harbor地址:
192.168.160.125:11180   admin  Harbor12345


mysql：  检查数据库防火墙
192.168.160.14
192.168.160.15
192.168.160.16
mysql -uroot -h 127.0.0.1 -p     root/Zjzb123!@#


机器ip：
192.168.160.28   k8s-pre-node01     master
192.168.160.29   k8s-pre-node02     master
192.168.160.30   k8s-pre-node03     master
192.168.160.31   k8s-pre-node04
192.168.160.32   k8s-pre-node05
192.168.160.33   k8s-pre-node06
192.168.160.34   k8s-pre-node07


sh installk8s.sh init --master 192.168.160.28 \
    --master 192.168.160.29 \
    --master 192.168.160.30 \
    --node 192.168.160.31 \
    --node 192.168.160.32 \
    --node 192.168.160.33 \
    --node 192.168.160.34 \
    --version v1.19.0 \
    --pkg-url /data/k8s-install/kube1.19.0.tar.gz

============================ k8s 生产 ==============================
mysql:
192.168.161.3:3306   主   root/Zjzb123!@#      登录方式：mysql -uroot -h127.0.0.1 -p
192.168.161.4:3306
192.168.161.5:3306

harbor地址:
192.168.160.125:11180   admin  Harbor12345


192.168.161.17    k8s-product-node01     master
192.168.161.18    k8s-product-node02     master
192.168.161.19    k8s-product-node03     master
192.168.161.20    k8s-product-node04
192.168.161.21    k8s-product-node05
192.168.161.22    k8s-product-node06
192.168.161.23    k8s-product-node07

sh installk8s.sh init --master 192.168.161.17 \
    --master 192.168.161.18 \
    --master 192.168.161.19 \
    --node 192.168.161.20 \
    --node 192.168.161.21 \
    --node 192.168.161.22 \
    --node 192.168.161.23 \
    --version v1.19.0 \
    --pkg-url /data/k8s-install/kube1.19.0.tar.gz


============================ NFS  beginning ==============================
测试环境:
192.168.160.11	NFS磁盘共享

#预生产
192.168.160.35	NFS磁盘共享

#生产环境
192.168.161.24	NFS磁盘共享


----------------------------------------------------

systemctl stop firewalld

unzip nfs.zip
cd nfs
sudo rpm -ivh *.rpm --force --nodeps

sudo su -
mkdir -p /data/nfsdata/ca
mkdir -p /data/nfsdata/cupload
mkdir -p /data/nfsdata/cuploadarch1
mkdir -p /data/nfsdata/cuploadarch2
mkdir -p /data/nfsdata/log
mkdir -p /data/nfsdata/mariadb
mkdir -p /data/nfsdata/mysql
mkdir -p /data/nfsdata/rabbitmq
mkdir -p /data/nfsdata/redis

vim  /etc/exports
/data/nfsdata/cupload *(rw,no_root_squash,sync)
/data/nfsdata/cuploadarch1 *(rw,no_root_squash,sync)
/data/nfsdata/cuploadarch2 *(rw,no_root_squash,sync)
/data/nfsdata/log *(rw,no_root_squash,sync)
/data/nfsdata/ca *(rw,no_root_squash,sync)


systemctl restart rpcbind
systemctl restart nfs

检查nfs是否正常
systemctl status nfs
showmount -e

nfs配置文件增加挂载路径后需要server端执行
exportfs -r



在k8s集群中的 所有work 节点上操作：
/data/install-packages/nfs-client
rpm -ivh *.rpm --force --nodeps

vim /etc/fstab

192.168.161.24:/data/nfsdata/ca /app/credit/ca  nfs     defaults        0       0
192.168.161.24:/data/nfsdata/cupload     /cupload        nfs     defaults        0       0
192.168.161.24:/data/nfsdata/cuploadarch2     /cuploadarch2         nfs     defaults        0       0
192.168.161.24:/data/nfsdata/cuploadarch1     /cuploadarch1        nfs     defaults        0       0
192.168.161.24:/data/nfsdata/log     /app/log        nfs     defaults        0       0


mkdir -p /app/credit/ca
mkdir -p /cupload
mkdir /cuploadarch2
mkdir /cuploadarch1
mkdir /app/log

mount -a

#验证
在任意work节点上某个目录下创建个文件，然后查看nfs服务器上对于的目录下是否同步。
例如,在任意一个work节点上：
echo 'zxl' >> /app/log/test.txt

在nfs server端查看对应的同步目录
cat /data/nfsdata/log/test.txt   #内容同样为 zxl



============================ NFS 生产 ending =================================



================================= CDH 5.16.2 测试 ==============================
192.168.160.79    hadoop-test-01      master
192.168.160.80    hadoop-test-02
192.168.160.81    hadoop-test-03
192.168.160.82    hadoop-test-04
192.168.160.83    hadoop-test-05

------------------

各个节点执行：
    配置 /etc/hosts

    配置jdk环境

    关闭防火墙
    sudo systemctl stop firewalld
    sudo systemctl disable firewalld

    关闭SELINUX
    sudo vi /etc/selinux/config
    SELINUX=disabled

1.5.	cloudera-manager安装

在主节点执行：
    解压CDH5.16.2.tar.gz

    scp cloudera-manager-centos7-cm5.16.2_x86_64.tar.gz app@192.168.160.80:/data/
    scp cloudera-manager-centos7-cm5.16.2_x86_64.tar.gz app@192.168.160.81:/data/
    scp cloudera-manager-centos7-cm5.16.2_x86_64.tar.gz app@192.168.160.82:/data/
    scp cloudera-manager-centos7-cm5.16.2_x86_64.tar.gz app@192.168.160.83:/data/

各个节点执行：
    tar cloudera-manager-centos7-cm5.16.2_x86_64.tar.gz
    vim cm-5.16.2/etc/cloudera-scm-agent/config.ini
        修改 server_host=hadoop-test-01

在主节点执行：
    cp CDH5.16.2/CDH-5.16.2-1.cdh5.16.2.p0.8-el7.parcel /data/cloudera/parcel-repo/
    cp CDH5.16.2/CDH-5.16.2-1.cdh5.16.2.p0.8-el7.parcel.sha1 /data/cloudera/parcel-repo/CDH-5.16.2-1.cdh5.16.2.p0.8-el7.parcel.sha
    cp CDH5.16.2/manifest.json /data/cloudera/parcel-repo/
    ll /data/cloudera/parcel-repo/
        -rwxr-x--- 1 app app 2132782197 Nov 27 15:30 CDH-5.16.2-1.cdh5.16.2.p0.8-el7.parcel
        -rwxr-x--- 1 app app         40 Dec  1 09:46 CDH-5.16.2-1.cdh5.16.2.p0.8-el7.parcel.sha
        -rwxr-x--- 1 app app      66804 Dec  1 09:47 manifest.json



1.6.	创建cloudera-manager用户(到此未执行)





================================ C-RocketMQ 多master集群模式 beginning ================================
--------------
#预生产
192.168.160.5  
192.168.160.77
192.168.160.78

序号	IP地址	     主机名   用户	           角色	                模式
1	192.168.160.5 	redis1	root	nameServer1,brokerServer1	Master1
2	192.168.160.77	redis2	root	nameServer2,brokerServer2	Master2
3	192.168.160.78	redis3	root	nameServer3,brokerServer3	Master3


-------------------
#生产
192.168.161.12
192.168.161.51
192.168.161.52

序号	IP地址	     主机名   用户	           角色	                模式
1	192.168.161.12 	redis1	root	nameServer1,brokerServer1	Master1
2	192.168.161.51	redis2	root	nameServer2,brokerServer2	Master2
3	192.168.161.52	redis3	root	nameServer3,brokerServer3	Master3


/etc/hosts文件内容如下：
192.168.161.12    redis-rocketmq-node1
192.168.161.51    redis-rocketmq-node2
192.168.161.52    redis-rocketmq-node3

192.168.161.12    rocketmq-broker-m1
192.168.161.51    rocketmq-broker-m2
192.168.161.52    rocketmq-broker-m3

---------------------------------------------------------
预生产示例如下：

配置hosts
    192.168.160.5   redis1
    192.168.160.77  redis2
    192.168.160.78  redis3

    192.168.160.5     rocketmq-broker-m1
    192.168.160.77    rocketmq-broker-m2
    192.168.160.78    rocketmq-broker-m3

关闭防火墙:
    sudo systemctl stop firewalld
    sudo systemctl disable firewalld
    sudo systemctl status firewalld

配置jdk环境：
sudo vim /etc/profile
    export JAVA_HOME=/data/java
    export PATH=$PATH:${JAVA_HOME}/bin

    ll /usr/bin/java
    rm /usr/bin/java
    sudo ln -s  /data/java/bin/java  /usr/bin/java

在/data目录下:
    unzip rocketmq-all-4.7.1-bin-release.zip
    mv rocketmq-all-4.7.1-bin-release rocketmq

酌情修改启动占用内存:
vim /data/rocketmq/bin/runbroker.sh   #(此环境未做修改,默认值如下)
    JAVA_OPT="${JAVA_OPT} -server -Xms8g -Xmx8g -Xmn4g"

#创建日志目录
mkdir -p /data/logs
替换日志目录配置:(把xml文件中的 '${user.home}' 替换为 '/data')
sed -i  's#${user.home}#/data#g'  /data/rocketmq/conf/*.xml

#创建数据目录:
mkdir -p /data/mq-store
mkdir -p /data/mq-store/commitlog
mkdir -p /data/mq-store/consumequeue
mkdir -p /data/mq-store/index
mkdir -p /data/mq-store/abort

修改broker启动配置文件：
/data/rocketmq/conf/2m-noslave  **注意配置文件目录**

192.168.160.5
vim /data/rocketmq/conf/2m-noslave/broker-a.properties

192.168.160.77
vim /data/rocketmq/conf/2m-noslave/broker-b.properties

192.168.160.78
vim /data/rocketmq/conf/2m-noslave/broker-c.properties

--------------------------------
brokerClusterName=rocketmq-3m-cluster
brokerName=rocketmq-broker-m1  ########## rocketmq-broker-m2 或者 rocketmq-broker-m3 #############
brokerId=0
namesrvAddr=192.168.160.5:9876;192.168.160.77:9876;192.168.160.78:9876
defaultTopicQueueNums=6
messageIndexSafe=true
autoCreateTopicEnable=true
autoCreateSubscriptionGroup=true
waitTimeMillsInSendQueue=5000
listenPort=10911
deleteWhen=04
fileReservedTime=168
#单个conmmitlog文件大小默认1GB
mapedFileSizeCommitLog=1073741824
mapedFileSizeConsumeQueue=6000000
#commitlog目录所在分区的最大使用比例，如果commitlog目录所在的分区使用比例大于该值，则触发过期文件删除
diskMaxUsedSpaceRatio=88
#默认允许的最大消息体默认4M,4194304
#maxMessageSize=65536
sendMessageThreadPoolNums=4
pullMessageThreadPoolNums=4
useReentrantLockWhenPutMessage=true
defaultReadQueueNums=16
defaultWriteQueueNums=16
storePathRootDir=/data/mq-store
storePathCommitLog=/data/mq-store/commitlog
storePathConsumeQueue=/data/mq-store/consumequeue
storePathIndex=/data/mq-store/index
storeCheckpoint=/data/mq-store/checkpoint
abortFile=/data/mq-store/abort
brokerRole=ASYNC_MASTER
#刷盘方式,默认为 ASYNC_FLUSH(异步刷盘),可选值SYNC_FLUSH(同步刷盘)
flushDiskType=ASYNC_FLUSH
--------------------------------------------------
#启动
cd /data/rocketmq/bin
nohup sh mqnamesrv >> mqnamesrv.log 2>&1 &
nohup sh mqbroker -c /data/rocketmq/conf/2m-noslave/broker-a.properties >> mqbroker.log 2>&1 &

nohup sh mqnamesrv >> mqnamesrv.log 2>&1 &
nohup sh mqbroker -c /data/rocketmq/conf/2m-noslave/broker-b.properties >> mqbroker.log 2>&1 &

nohup sh mqnamesrv >> mqnamesrv.log 2>&1 &
nohup sh mqbroker -c /data/rocketmq/conf/2m-noslave/broker-c.properties >> mqbroker.log 2>&1 &

#验证nameserver
ps -ef | grep mqnamesrv
netstat -ntlp | grep 9876

#验证broker
ps -ef | grep broker
netstat -ntlp | grep 10911

#关闭broker
sh mqshutdown broker

#关闭nameserver
sh mqshutdown namesrv
================================ C-RocketMQ 多master集群 ending ================================



================================ B-RocketMQ 主从集群模式 beginning ================================
#生产环境:
192.168.161.33
192.168.161.34

192.168.161.33    rocketmq-node1
192.168.161.34    rocketmq-node2

192.168.161.33    rocketmq-nameserver1
192.168.161.33    rocketmq-master1
192.168.161.34    rocketmq-nameserver2
192.168.161.34    rocketmq-master1-slave

关闭防火墙:
    sudo systemctl stop firewalld
    sudo systemctl disable firewalld
    sudo systemctl status firewalld

配置jdk环境：
sudo vim /etc/profile
    export JAVA_HOME=/data/java
    export PATH=$PATH:${JAVA_HOME}/bin

    ll /usr/bin/java
    rm /usr/bin/java
    sudo ln -s  /data/java/bin/java  /usr/bin/java

在/data目录下:
    unzip rocketmq-all-4.7.1-bin-release.zip
    mv rocketmq-all-4.7.1-bin-release rocketmq

酌情修改启动占用内存:
vim /data/rocketmq/bin/runbroker.sh   #(此环境未做修改,默认值如下)
    JAVA_OPT="${JAVA_OPT} -server -Xms8g -Xmx8g -Xmn4g"

#创建日志目录
mkdir -p /data/logs
替换日志目录配置:(把xml文件中的 '${user.home}' 替换为 '/data')
sed -i  's#${user.home}#/data#g'  /data/rocketmq/conf/*.xml

#创建数据目录:
mkdir -p /data/mq-store
mkdir -p /data/mq-store/commitlog
mkdir -p /data/mq-store/consumequeue
mkdir -p /data/mq-store/index
mkdir -p /data/mq-store/abort

在192.168.161.33机器上:
vim /data/rocketmq/conf/2m-2s-async/broker-a.properties

brokerClusterName=pulian-rocketmq-cluster
brokerName=broker-a
brokerId=0
namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876
defaultTopicQueueNums=4
autoCreateTopicEnable=true
autoCreateSubscriptionGroup=true
listenPort=10911
deleteWhen=04
fileReservedTime=168
mapedFileSizeCommitLog=1073741824
mapedFileSizeConsumeQueue=300000
diskMaxUsedSpaceRatio=88
storePathRootDir=/data/mq-store
storePathCommitLog=/data/mq-store/commitlog
storePathConsumeQueue=/data/mq-store/consumequeue
storePathIndex=/data/mq-store/index
storeCheckpoint=/data/mq-store/checkpoint
abortFile=/data/mq-store/abort
maxMessageSize=65536
brokerRole=ASYNC_MASTER
flushDiskType=ASYNC_FLUSH

在192.168.161.34机器上:
vim /data/rocketmq/conf/2m-2s-async/broker-a-s.properties

#所属集群名字
brokerClusterName=pulian-rocketmq-cluster
brokerName=broker-a
brokerId=1
namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876
defaultTopicQueueNums=4
autoCreateTopicEnable=true
autoCreateSubscriptionGroup=true
listenPort=10911
deleteWhen=04
fileReservedTime=120
mapedFileSizeCommitLog=1073741824
mapedFileSizeConsumeQueue=300000
diskMaxUsedSpaceRatio=88
storePathRootDir=/data/mq-store
storePathCommitLog=/data/mq-store/commitlog
storePathConsumeQueue=/data/mq-store/consumequeue
storePathIndex=/data/mq-store/index
storeCheckpoint=/data/mq-store/checkpoint
abortFile=/data/mq-store/abort
maxMessageSize=65536
brokerRole=SLAVE
flushDiskType=ASYNC_FLUSH


sh mqnamesrv >> mqnamesrv.log 2>&1 &
nohup sh mqbroker -c /data/rocketmq/conf/2m-2s-async/broker-a.properties >> mqbroker.log 2>&1 &

sh mqnamesrv >> mqnamesrv.log 2>&1 &
nohup sh mqbroker -c /data/rocketmq/conf/2m-2s-async/broker-a-s.properties >> mqbroker.log 2>&1 &


#验证nameserver
ps -ef | grep mqnamesrv
netstat -ntlp | grep 9876

#验证broker
ps -ef | grep broker
netstat -ntlp | grep 10911


#关闭broker
sh mqshutdown broker

#关闭nameserver
sh mqshutdown namesrv

================================ B-RocketMQ 主从集群模式 ending ================================



================================ B-elasticsearch 集群 beginning ================================
暂时关闭防火墙
sudo systemctl stop firewalld
sudo systemctl disable firewalld
sudo sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
sudo setenforce 0



sudo vim /etc/hosts
192.168.161.40  node1
192.168.161.41  node2
192.168.161.42  node3



sudo vim /etc/profile
export JAVA_HOME=/data/java
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin

#检查NTP服务是否正常
ntpq -p

#修改系统参数
sudo echo '* soft nofile 655360'>> /etc/security/limits.conf
sudo echo '* hard nofile 655360'>> /etc/security/limits.conf
sudo echo '* hard nproc  40960'>> /etc/security/limits.conf
sudo echo '* soft nproc  40960'>> /etc/security/limits.conf
sudo echo 'app soft memlock unlimited' >> /etc/security/limits.conf
sudo echo 'app hard memlock unlimited' >> /etc/security/limits.conf


sudo echo 'vm.swappiness=1' >> /etc/sysctl.conf
sudo echo 'vm.max_map_count=262144' >> /etc/sysctl.conf

#sysctl配置生效
sysctl -p

#用户退出再登录limits.conf参数生效

mkdir -p /data/es-data/data
mkdir -p /data/es-data/logs

vim /data/elasticsearch/config/jvm.options
-Xms16g
-Xmx16g

#192.168.161.40  node1 上
vim /data/elasticsearch/config/elasticsearch.yml

cluster.name: zjzb-pl-cluster
node.name: node1
node.master: true
node.data: true
node.max_local_storage_nodes: 3
path.data: /data/es-data/data
path.logs: /data/es-data/logs
network.host: node1
http.port: 9200
transport.tcp.port: 9300
cluster.initial_master_nodes: ["node1"]
http.cors.enabled: true
http.cors.allow-origin: "*"
cluster.routing.allocation.same_shard.host: true
bootstrap.memory_lock: true
gateway.recover_after_master_nodes: 2
gateway.recover_after_data_nodes: 2
discovery.zen.ping.unicast.hosts: ["node1", "node2","node3"]

---------------
#192.168.161.41  node2 上
vim /data/elasticsearch/config/elasticsearch.yml

cluster.name: zjzb-pl-cluster
node.name: node2
node.master: true
node.data: true
node.max_local_storage_nodes: 3
path.data: /data/es-data/data
path.logs: /data/es-data/logs
network.host: node2
http.port: 9200
transport.tcp.port: 9300
cluster.initial_master_nodes: ["node1"]
http.cors.enabled: true
http.cors.allow-origin: "*"
cluster.routing.allocation.same_shard.host: true
bootstrap.memory_lock: true
gateway.recover_after_master_nodes: 2
gateway.recover_after_data_nodes: 2
discovery.zen.ping.unicast.hosts: ["node1", "node2","node3"]

-----------
#192.168.161.42  node3 上
vim /data/elasticsearch/config/elasticsearch.yml
#大致内容同node2


/data/elasticsearch/bin/elasticsearch -d

================================ B-elasticsearch 集群 ending ================================

================================ C-redis beginning ================================
#生产环境

192.168.161.12    redis-rocketmq-node1         ***(redis master)
192.168.161.51    redis-rocketmq-node2         ***(redis slave0)
192.168.161.52    redis-rocketmq-node3         ***(redis slave1)

192.168.161.12    rocketmq-broker-m1
192.168.161.51    rocketmq-broker-m2
192.168.161.52    rocketmq-broker-m3

------------------------------------------


redis集群安装

root操作
firewall-cmd --zone=public --add-port=26379/tcp --permanent
firewall-cmd --zone=public --add-port=6379/tcp --permanent
firewall-cmd --reload 
firewall-cmd --zone=public --list-ports

创建操作系统用户(root)

groupadd redis && useradd -g redis redis
passwd redis   ###（根据环境自定义密码） Passc0de!


mkdir -p /opt/redis
chown -R redis:redis /opt/redis

修改系统参数：
vi /etc/sysctl.conf

添加一行：
vm.overcommit_memory = 1

使系统参数生效
sysctl -p

-----------

临时方法：
echo never > /sys/kernel/mm/transparent_hugepage/enabled 
永久方法：
将上面一条命令写入rc.local文件
vi /etc/rc.d/rc.local
echo never > /sys/kernel/mm/transparent_hugepage/enabled

chmod 777 /etc/rc.d/rc.local

备注：必须用root用户执行 没有权限
-----------

切换到redis用户

# su - redis
cd /opt/redis

mkdir -p /opt/redis/data && mkdir -p /opt/redis/app

mkdir -p /opt/redis/data/redis1/ && mkdir -p /opt/redis/data/redis  &&  mkdir -p /opt/redis/data/sentinel

cd /opt/redis/app
上传安装包至当前目录
tar -xvf redis-5.0.10.tar.gz
cd /opt/redis/app/redis-5.0.10/
make  (使用当前Makefile编译文件)  
    #编译时报错 /bin/sh: cc: command not found
    yum -y install gcc gcc-c++ libstdc++-devel

    #编译时报错:fatal error: jemalloc/jemalloc.h: No such file or directory
    yum install jemalloc
        #rpm -ql jemalloc 查看jemalloc库的位置
    make MALLOC=/usr/lib64/libjemalloc.so.1
make test
    #报错:You need tcl 8.5 or newer in order to run the Redis test
    yum install -y tcl
使用示例配置文件生成：
cat  /opt/redis/app/redis-5.0.10/redis.conf | grep -v "#" |grep -v "^$" > /opt/redis/app/redis-5.0.10/redis1.conf（备注：此步骤可删除）

cd /opt/redis/data/redis

vim /opt/redis/data/redis/redis.conf

！！！！！！注意 slave节点需要把备注的部分放开 ！！！！！！！
##############Master################  修改bind
bind 192.168.161.12
protected-mode yes
port 6379
requirepass 123456
masterauth 123456
maxmemory 2GB
tcp-backlog 511
timeout 0
tcp-keepalive 300
daemonize no
supervised no
pidfile /var/run/redis_1.pid
loglevel notice
logfile "/opt/redis/data/redis1/redis.log"
databases 16
always-show-logo yes
save 10 1
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
dir /opt/redis/data/redis1/ 
# 修改以下两个参数作为slave节点(备注：如果是slave节点,以下两行注释请去掉,replicaof改成master节点的IP )           
# replicaof 12.0.217.157 6379
# masterauth 123456
replica-serve-stale-data yes
replica-read-only yes
repl-diskless-sync no
repl-diskless-sync-delay 5
repl-disable-tcp-nodelay no
replica-priority 100
lazyfree-lazy-eviction no
lazyfree-lazy-expire no
lazyfree-lazy-server-del no
replica-lazy-flush no
appendonly yes                
appendfilename "appendonly.aof"
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
aof-load-truncated yes
aof-use-rdb-preamble yes
lua-time-limit 5000
slowlog-log-slower-than 10000
slowlog-max-len 128
latency-monitor-threshold 0
notify-keyspace-events ""
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-size -2
list-compress-depth 0
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64
hll-sparse-max-bytes 3000
stream-node-max-bytes 4096
stream-node-max-entries 100
activerehashing yes
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit replica 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
hz 10
dynamic-hz yes
aof-rewrite-incremental-fsync yes
rdb-save-incremental-fsync yes

-------------------------------
Salve： 修改bind和replicaof

#注意bind ip要修改
bind 192.168.161.51
protected-mode yes
port 6379
requirepass 123456
masterauth 123456
maxmemory 2GB
tcp-backlog 511
timeout 0
tcp-keepalive 300
daemonize no
supervised no
pidfile /var/run/redis_1.pid
loglevel notice
logfile "/opt/redis/data/redis1/redis.log"
databases 16
always-show-logo yes
save 10 1
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
dir /opt/redis/data/redis1/
# 修改以下两个参数作为slave节点(备注：158, 159注释去掉 改成master节点的IP ) 
replicaof 192.168.161.12 6379
masterauth 123456
replica-serve-stale-data yes
replica-read-only yes
repl-diskless-sync no
repl-diskless-sync-delay 5
repl-disable-tcp-nodelay no
replica-priority 100
lazyfree-lazy-eviction no
lazyfree-lazy-expire no
lazyfree-lazy-server-del no
replica-lazy-flush no
appendonly yes 
appendfilename "appendonly.aof"
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
aof-load-truncated yes
aof-use-rdb-preamble yes
lua-time-limit 5000
slowlog-log-slower-than 10000
slowlog-max-len 128
latency-monitor-threshold 0
notify-keyspace-events ""
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-size -2
list-compress-depth 0
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64
hll-sparse-max-bytes 3000
stream-node-max-bytes 4096
stream-node-max-entries 100
activerehashing yes
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit replica 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
hz 10
dynamic-hz yes
aof-rewrite-incremental-fsync yes
rdb-save-incremental-fsync yes

非root启动redis服务器进程:
/opt/redis/app/redis-5.0.10/src/redis-server /opt/redis/data/redis/redis.conf &

#验证:
netstat -ntlp | grep 6379


三个节点都需要修改sentinel.conf
vim /opt/redis/data/sentinel/sentinel.conf
sentinel.conf内容：
#########################################
port 26379
daemonize no
protected-mode no
pidfile /var/run/redis-sentinel.pid
logfile "/opt/redis/data/sentinel/sentinel.log"
dir /opt/redis/data/sentinel/
sentinel monitor zjzbMaster 192.168.161.12 6379 2
sentinel auth-pass zjzbMaster 123456
sentinel down-after-milliseconds zjzbMaster 30000
sentinel parallel-syncs zjzbMaster 1
sentinel failover-timeout zjzbMaster 180000
sentinel deny-scripts-reconfig yes
########################################

非root启动哨兵进程:
/opt/redis/app/redis-5.0.10/src/redis-server /opt/redis/data/sentinel/sentinel.conf --sentinel &

##问题: 集群一开始没起来 需要把redis.conf和sentinel.conf全部改成主节点的

连接到主节点  能看到两个从节点
/opt/redis/app/redis-5.0.10/src/redis-cli -h 192.168.161.12 -p 6379
auth 123456
info replication

================================ C-redis ending ================================







================================ A 生产环境 Mongodb 集群 beginning ================================
机器：
192.168.161.9:37017
192.168.161.10:37017
192.168.161.11:37017

连接集群：
mongo --host jxrtRS/192.168.161.9:37017,192.168.161.10:37017,192.168.161.11:37017 -uccbscf_app -p ccbscf@M0NG0 --authenticationDatabase admin

# 使用mongo客户端连接relica set进行测试
mongo --host jxrtRS/192.168.161.9:37017,192.168.161.10:37017,192.168.161.11:37017 -uroot -p r00t@M0NG0 --authenticationDatabase admin


###与科创MongoDB安装手册一致###

/data/mongodb/conf
/data/mongodb/data
/data/mongodb/log
/data/mongodb/pid

#安装目录脚本,已配置环境变量
/opt/mongodb/bin/


db.auth('root', 'r00t@M0NG0')
db.auth('rsadmin', 'rs@dmin@M0NG0')
db.auth('ccbscf_app', 'ccbscf@M0NG0')



=========================C 大智慧=================================
firewall-cmd --zone=public --add-port=8801/tcp --permanent
firewall-cmd --zone=public --add-port=5000/tcp --permanent

sudo service mysqld start


firewall-cmd --zone=public --permanent --remove-rich-rule="rule family="ipv4" source address="10.1.30.135" accept"

firewall-cmd --reload
firewall-cmd --list-port
----
开发环境10.1.20.53
数据库:
10.1.20.68
root/Dev123!@#
fcdb/fcdb  finchina
----
预生产环境:192.168.160.23
数据库:
root/Zjzb123!@#
fcdb/fcdb  finchina
create database finchina default character set utf8 collate utf8_general_ci;
CREATE USER fcdb@'%' IDENTIFIED BY 'fcdb';
GRANT ALL PRIVILEGES ON finchina.* TO 'fcdb'@'%' WITH GRANT OPTION;
flush privileges;

firewall-cmd --zone=public --add-port=8801/tcp --permanent
firewall-cmd --zone=public --add-port=5000/tcp --permanent
firewall-cmd --zone=public --add-port=3306/tcp --permanent

----
生产环境:192.168.161.98
数据库:
root/Zjzb123!@#
fcdb/fcdb  finchina
create database finchina default character set utf8 collate utf8_general_ci;
CREATE USER fcdb@'%' IDENTIFIED BY 'fcdb';
GRANT ALL PRIVILEGES ON finchina.* TO 'fcdb'@'%' WITH GRANT OPTION;
flush privileges;

firewall-cmd --zone=public --add-port=8801/tcp --permanent
firewall-cmd --zone=public --add-port=5000/tcp --permanent
firewall-cmd --zone=public --add-port=3306/tcp --permanent

-------

sudo systemctl stop firewalld

#配置hosts,以防配置中含有此域名
222.73.12.20    entapi.finchina.com

关闭bin-log,重启mysql
sudo service mysqld stop
sudo service mysqld status
sudo service mysqld start

mysql> show variables like 'log_bin';  #OFF


cd /data/install-packages/finchina/FCDT3.5_LINUX-mysql/fcdtdown
命令：
nohup ./fcdtdown > /data/install-packages/finchina/FCDT3.5_LINUX-mysql/fcdtdown/fcdtdown-nohup.log 2>&1 &

查看日志:
/data/install-packages/finchina/FCDT3.5_LINUX-mysql/fcdtdown/logs/
moddownflow.log


cd /data/install-packages/finchina/ent-server/ent2.1.12_linux_x64/entService_linux-x64
nohup dotnet entService.dll > entService-nohup.log 2>&1 &
查看日志:
/data/install-packages/finchina/ent-server/ent2.1.12_linux_x64/entService_linux-x64/logs


cd /data/install-packages/finchina/ent-server/ent2.1.12_linux_x64/entConsole_linux-x64
nohup dotnet entConsole.dll --urls=http://*:5000 > entConsole-nohup.log 2>&1 &



=====================================环境配置==================================
yum源: 测试、预生产？、生产已打通
yum源地址改为： http://192.168.160.125:8080/local-yum/

